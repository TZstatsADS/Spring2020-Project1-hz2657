---
title: "Data Prepration"
author: "Huizhe (Sunny) ZHU"
output: html_notebook
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


### Step 0 - Load libraries
Install packages & Load libraries
```{r load libraries, message=FALSE, warning=FALSE}
install.packages('broom')
library(tm)
library(data.table)
library(tidytext)
library(tidyverse)
library(DT)
```

### Step 1 - Load the data
```{r}
# load lyrics data
library(readr)
raw_lyrics <- read.csv("lyrics.csv", stringsAsFactors = FALSE)
```


### Step 2 - Preliminary cleaning of text

Convert letters to the lower case, remove punctuation, numbers, empty words and extra white space.
```{r text processing in tm}
# function for removimg leading and trailing whitespace from character strings 
leadingWhitespace <- content_transformer(function(x) str_trim(x, side = "both"))

# remove stop words
data("stop_words")
word <- c("lot", "today", "months", "month", "wanna", "wouldnt", "wasnt", "ha", "na", "ooh", "da",
        "gonna", "im", "dont", "aint", "wont", "yeah", "la", "oi", "nigga", "fuck",
          "hey", "year", "years", "last", "past", "feel")
stop_words <- c(stop_words$word, word)

```


```{r}
# clean the data and make a corpus
corpus <- VCorpus(VectorSource(raw_lyrics$lyrics))%>%
  tm_map(content_transformer(tolower))%>%
  tm_map(removePunctuation)%>%
  tm_map(removeWords, character(0))%>%
  tm_map(removeWords, stop_words)%>%
  tm_map(removeNumbers)%>%
  tm_map(stripWhitespace)%>%
  tm_map(leadingWhitespace)
```


### Step 3 - Stemming and converting to tidy object

Stemming reduces a word to its word *stem*. Each row contains 1 song - its stemmed words
* Note: 11 empty lines 
```{r stemming}
stemmed <- tm_map(corpus, stemDocument) %>%
  tidy() %>%
  select(text)
```


### Step 4 - Tokenization on original words

*9.51* million rows (words) in total, one column
```{r tidy dictionary}
dict <- tidy(corpus) %>%
  select(text) %>%
  unnest_tokens(dictionary, text)
```


### Step 5 - Tokenization on stemmed words

Add ID = row_number, dict = original word

*9.51* million rows, 3 columns
```{r tidy stems with dictionary}
completed <- stemmed %>%
  mutate(id = row_number()) %>%
  unnest_tokens(stems, text) %>%
  bind_cols(dict) 
```


### Step 6 - Stem completion

Picking the original word (from the same root) with the highest frequency. The new column created -  'word' - will be used to replace stem, which is sometimes hard for people to understand. 

```{r stem completion, warning=FALSE, message=FALSE}
completed1 <- completed %>%
  group_by(stems) %>%
  count(dictionary) %>%
  mutate(word = dictionary[which.max(n)]) %>%
  ungroup() %>%
  select(stems, word) %>%
  distinct() %>%
  right_join(completed) %>%
  select(-stems)
```


### Step 7 - Pasting stem completed individual words into their respective lyrics

Put 'word' generated from Step 6, back to each song row - each row contains words like before - but the words are transforme, from -> stemmed -> completed stem
```{r reverse unnest}
completed2 <- completed1 %>%
  group_by(id) %>%
  summarise(stemmedwords= str_c(word, collapse = " ")) %>%
  ungroup()
```


### Step 8 - Keeping a track of the processed lyrics with their own ID
```{r cleaned hm_data, warning=FALSE, message=FALSE}
raw_lyrics1 <- raw_lyrics %>%
  mutate(id = row_number()) %>%
  inner_join(completed2)
```


### Exporting the processed text data into a CSV file
```{r export data}
save(raw_lyrics1, file="../output/processed_lyrics1.RData")
```

The final processed data is ready to be used for any kind of analysis.
