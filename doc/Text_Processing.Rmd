---
title: 'Listen: Pop or Country, Hiphop or R&B'

author: "Huizhe (Sunny) ZHU"
output:
  html_notebook: default
  html_document:
    df_print: paged
  pdf_document: default
---
\
\
![](https://api.time.com/wp-content/uploads/2018/04/listening-to-music-headphones.jpg?w=800&quality=85)
\

People on internet are arguing about the difference between Country and Pop songs, Hiphop and R&B songs. Pop represents 'popular', while country music is also popular in many regions including the U.S., which makes them so similar to each other. Meanwhile, Rhythm & Blues (R&B) is considered as a mix of Pop and Hip-Hop songs, thus it is also difficult to distinguish Hiphop and R&B. 

Given the 'lyrics' database that contains more than 120,000 songs, we here have the chance to identify the differences among these genres by real examples. 


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



```{r load libraries, message=FALSE, warning=FALSE, include = FALSE}
#### Step 0 - Install packages & Load libraries
install.packages('broom')
install.packages('hrbrthemes')
install.packages('viridis')

library(tm)
library(data.table)
library(tidytext)
library(tidyverse)
library(DT)
library(hrbrthemes)
library(viridis)
```


```{r, include = FALSE}
#### Step 1 - Load the data
#library(readr)
#raw_lyrics <- read.csv("lyrics.csv", stringsAsFactors = FALSE)
```

```{r,include = FALSE}
load('../data/lyrics.RData') 
raw_lyrics = dt_lyrics
```



```{r text processing in tm, include = FALSE}
#### Step 2 - Preliminary cleaning of text
#Convert letters to the lower case, remove punctuation, numbers, empty words and extra white space.

leadingWhitespace <- content_transformer(function(x) str_trim(x, side = "both"))

# remove stop words
data("stop_words")
word <- c("lot", "today", "months", "month", "wanna", "wouldnt", "wasnt", "ha", "na", "ooh", "da",
        "gonna", "im", "dont", "aint", "wont", "yeah", "la", "oi", "nigga", "fuck",
          "hey", "year", "years", "last", "past", "feel")
stop_words <- c(stop_words$word, word)

```


```{r,include = FALSE}
# clean the data and make a corpus
corpus <- VCorpus(VectorSource(raw_lyrics$lyrics))%>%
  tm_map(content_transformer(tolower))%>%
  tm_map(removePunctuation)%>%
  tm_map(removeWords, character(0))%>%
  tm_map(removeWords, stop_words)%>%
  tm_map(removeNumbers)%>%
  tm_map(stripWhitespace)%>%
  tm_map(leadingWhitespace)
```


```{r stemming, include = FALSE}
#### Step 3 - Stemming and converting to tidy object
#Stemming reduces a word to its word *stem*. Each row contains 1 song - its stemmed words - Note: 11 empty lines 
stemmed <- tm_map(corpus, stemDocument) %>%
  tidy() %>%
  select(text)
```


```{r tidy dictionary,include = FALSE}
#### Step 4 - Tokenization on original words, *9.51* million rows (words) in total, one column
dict <- tidy(corpus) %>%
  select(text) %>%
  unnest_tokens(dictionary, text)
```


```{r tidy stems with dictionary,include = FALSE}
#### Step 5 - Tokenization on stemmed words  -  Add ID = row_number, dict = original word, 9.51 million rows, 3 columns
completed <- stemmed %>%
  mutate(id = row_number()) %>%
  unnest_tokens(stems, text) %>%
  bind_cols(dict) 
```



```{r stem completion, warning=FALSE, message=FALSE,include = FALSE}
#### Step 6 - Stem completion
#Picking the original word (from the same root) with the highest frequency. The new column created -  'word' - will be used to replace stem, which is sometimes hard for people to understand. 
completed1 <- completed %>%
  group_by(stems) %>%
  count(dictionary) %>%
  mutate(word = dictionary[which.max(n)]) %>%
  ungroup() %>%
  select(stems, word) %>%
  distinct() %>%
  right_join(completed) %>%
  select(-stems)
```


```{r reverse unnest, message=FALSE,include = FALSE}
#### Step 7 - Pasting stem completed individual words into their respective lyrics
#Put 'word' generated from Step 6, back to each song row - each row contains words like before - but the words are transforme, from -> stemmed -> completed stem
completed2 <- completed1 %>%
  group_by(id) %>%
  summarise(stemmedwords= str_c(word, collapse = " ")) %>%
  ungroup()
```


```{r cleaned hm_data, warning=FALSE, message=FALSE,include = FALSE}
#### Step 8 - Keeping a track of the processed lyrics with their own ID
raw_lyrics1 <- raw_lyrics %>%
  mutate(id = row_number()) %>%
  inner_join(completed2)
```


```{r export data,message=FALSE,include = FALSE}
#### Step 9 - Exporting the processed text data into a CSV file
save(raw_lyrics1, file="../output/processed_lyrics3.RData")
```


The lyrics dataset contains 7534 Country songs, 18697 Pop songs,  8905 Hip-Hops, and 2174 R&B songs. 

```{r,include = FALSE}
#### Step 1 - Create sub-dataset
table(raw_lyrics1$genre)
```


```{r,include = FALSE}
# subset the corresponding genres
country = raw_lyrics1[raw_lyrics1$genre == 'Country',]
pop = raw_lyrics1[raw_lyrics1$genre == 'Pop',]
hiphop = raw_lyrics1[raw_lyrics1$genre == 'Hip-Hop',]
RB = raw_lyrics1[raw_lyrics1$genre == 'R&B',]
```


\

#### 1. Compare lyrics length

According to the boxplot, we can see Pop music is longer than country songs on average, and has a larger variance. While country music tends to be shorter and has a smaller variance. 

```{r,echo=FALSE, message=FALSE, warning=FALSE}
pop_country = bind_rows(pop,country)

pop_country$word_count <- str_count(pop_country$lyrics, '\\s+')+1

pop_country <- pop_country[-c(621,1665),]

boxplot(word_count~genre,data=pop_country, main = "Lyrics Length between Country & Pop",col=(c("lightblue2","lavenderblush")),alpha=0.2, ylab="lyrics Length",   xlab="Genre",cex.lab=0.8, cex.axis=0.8, cex.main=1, cex.sub=0.5)
```

On average, Hip-Hop tends to be longer than R&B and has a larger variance, while R&B tends to be shorter and has smaller variance. 

```{r,echo=FALSE, message=FALSE, warning=FALSE}
hiphop_RB = bind_rows(hiphop,RB)

hiphop_RB$word_count <- str_count(hiphop_RB$lyrics, '\\s+')+1

hiphop_RB <- hiphop_RB[-c(3208,3211),]

boxplot(word_count~genre,data=hiphop_RB, main = "Lyrics Length between Hip-Hop & R&B",col=(c("lightblue2","lavenderblush")),alpha=0.8,   xlab="Genre", ylab="lyrics Length",cex.lab=0.8, cex.axis=0.8, cex.main=1, cex.sub=0.5)
```




```{r,echo=FALSE, message=FALSE, warning=FALSE}
library(dplyr)
library(ngram)
total_four_songs = bind_rows(pop,country,hiphop,RB)

library(stringr)
total_four_songs$word_count <- str_count(total_four_songs$lyrics, '\\s+')+1

# remove outliers - remove rows 
total_four_songs <- na.omit(total_four_songs)
total_four_songs <- total_four_songs[-c(29439,29442,33045,28849),]

```





#### 2. Compare high-frequency words

Next, let's analyze their corresponding high frequency words and use it to define its topics. 

First, it's an interesting phenomenon that the most frequent wordn in these four genres is love. 

Then, we can find that Pop and country have not much difference regarding frequeny words mentioned in lyrics. 

The most frequent words include Love, baby, heart, girl/boy, eyes, world.

Pop contains more words: 

Country songs contain more words: home, hand


#### 2.1 Pop
```{r,echo=FALSE, message=FALSE, warning=FALSE}
tidy_pop <- unnest_tokens(pop, output = 'word', token = 'words', input = stemmedwords)%>%
  count(word,sort = TRUE)

tidy_pop = tidy_pop[-c(3,6,10,23,33),]

# barplot
barplot(tidy_pop[1:30,]$n, las = 2, names.arg = tidy_pop[1:30,]$word,
        col ="pink", main ="Pop - Most frequent words",
        ylab = "Word frequencies")

```


```{r}
# scatter plot
par(mfrow=c(1,2))

tidy_pop1 = tidy_pop[1:50,]
ggplot(data=tidy_pop1, aes(x=word,y=n),color = tidy_pop1$word)+
  geom_point() +
  geom_text(label=tidy_pop1$word)

tidy_country1 = tidy_country[1:50,]

ggplot(data=tidy_country1, aes(x=word,y=n),color = tidy_country1$word)+
  geom_point() +
  geom_text(label=tidy_country1$word)


```





```{r,echo=FALSE, message=FALSE, warning=FALSE}
# word cloud for pop
install.packages('wordcloud')
library(wordcloud)

# Converting the text file into a Corpus
corpus_pop <- VCorpus(VectorSource(tidy_pop$word))

wordcloud (corpus_pop, scale=c(5,0.5), max.words=100, random.order=FALSE, 
           rot.per=0.35, use.r.layout=FALSE, colors=brewer.pal(18, 'Dark2'))
```



#### 2.2 Country
```{r,echo=FALSE, message=FALSE, warning=FALSE}
tidy_country <- unnest_tokens(country, output = 'word', token = 'words', input = stemmedwords)%>%
  count(word,sort = TRUE)

tidy_country = tidy_country[-c(3,4,6,15,29),]

tidy_country1 = tidy_country[1:50,]

# barplot
barplot(tidy_country[1:30,]$n, las = 3, names.arg = tidy_country[1:30,]$word,
        col ="light blue", main ="Country - Most frequent words",
        ylab = "Word frequencies")
```


```{r}
pop_country_50 = bind_rows(tidy_country1,tidy_pop1)

barplot(pop_country_50$n, las = 2, names.arg = pop_country_50$word, group=
        col ="light blue", main ="Country - Most frequent words",
        ylab = "Word frequencies")

```




#### 2.3 HipHops

HipHops and R&B's most frequent words is also Love. Other than that, they both include baby, time, girl.

HipHops contains more *** negative words:, 

R&B is more similar to pop/country songs regarding word frequency. 



```{r,echo=FALSE, message=FALSE, warning=FALSE}
tidy_hiphop <- unnest_tokens(hiphop, output = 'word', token = 'words', input = stemmedwords)%>%
  count(word,sort = TRUE)

# barplot
barplot(tidy_hiphop[1:30,]$n, las = 2, names.arg = tidy_hiphop[1:30,]$word,
        col ="orange", main ="Hiphop - Most frequent words",
        ylab = "Word frequencies")

```


#### 2.4 R&B
```{r,echo=FALSE, message=FALSE, warning=FALSE}
tidy_RB <- unnest_tokens(RB, output = 'word', token = 'words', input = stemmedwords)%>%
  count(word,sort = TRUE)

# barplot
barplot(tidy_RB[1:30,]$n, las = 2, names.arg = tidy_RB[1:30,]$word,
        col ="black", main ="R&B - Most frequent words",
        ylab = "Word frequencies")
```








### Step 4 - Compare their emotions

Now let's check on the sentiment!

Compare the sentiment score for these 4 different types of songs. 

Hip hop and R&B are more similar, tend to have stronger emotions Hip-hop tends to have more negative words that R&B. 

Country and Pop are more similar. Their emotions are more gentle, and they both contains balanced emotions. 


#can use scatter plot that transfer each point to its word meaning, larger word represent more frequency, 
merge them in 2 and 2 graphs

```{r, include=FALSE}
positive <- get_sentiments("bing") %>%
  filter(sentiment == "positive")

negative <- get_sentiments("bing") %>%
  filter(sentiment == "negative")
```

The top 10 positive words in Pop songs include: 
```{r,echo=FALSE, message=FALSE, warning=FALSE}
tidy_pop %>%
  semi_join(positive) 
```
The top 10 negative words in Pop songs include: 
```{r,echo=FALSE, message=FALSE, warning=FALSE}
library(dplyr)
tidy_pop_negative <- tidy_pop%>%
  semi_join(negative) 
head(tidy_pop_negative,10)
```

we count up how many positive and negative words there are in each genre. Letâ€™s find a sentiment score for each word  -- compare setiment along different genres

Referece:https://www.tidytextmining.com/sentiment.html from *Text mining with R*

# Sentiment for each genre 

```{r,echo=FALSE, message=FALSE, include=FALSE}
# Combine the first 400 songs of each genre

pop_400 = pop[1:400,]
country_400 = country[1:400,]
hiphop_400 = hiphop[1:400,]
RB_400 = RB[1:400,]

total_400 = bind_rows(pop_400,country_400,hiphop_400,RB_400)

tidy_total_400 <- unnest_tokens(total_400, output = 'word', token = 'words', input = stemmedwords)

```


From the diagram, we can see, Hip-hop and R&B tend to have stronger emotions, while country and pop are more gentle.

```{r,echo=FALSE, message=FALSE, warning=FALSE}
library(tidyr)
bing <- get_sentiments("bing")

songs_sentiment_try <- tidy_total_400 %>%
  inner_join(bing) %>%
  count(genre,song,sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)

ggplot(data = songs_sentiment_try, aes(x=song,y=sentiment, fill=genre,color=genre))+
  geom_bar(stat = "identity", show.legend = FALSE) +
  facet_wrap(~genre, ncol = 2, scales = "free_x")

```


# Summary 

Lyrics length:

Frequent words:

Emotion variation:


### References
Quora (2019) https://www.quora.com/How-do-you-differentiate-between-a-country-song-and-a-pop-song-just-by-listening-to-it


























